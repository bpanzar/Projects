{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEB SCRAPING\n",
    "\n",
    "***Disclaimer: Some of these website have since adjusted the permissions of robot.txt so these scrapers may not work since I did the project.***\n",
    "\n",
    "#### 1. Gather urls of movie content reviews\n",
    "\n",
    "First we will gather the urls of movie reviews from www.moviesguide.org.  This website reviews movies based on the content such as, language, violence, and nudity.   \n",
    "\n",
    "I used Scrapy because the website uses 'infinite scrolling' to display the movies that are reviewed.  The spider scrapes the urls of movies displayed on the page, then it requests an update to the website via the infinite scrolling mechanic, scrapes the next batch of urls, and continues until there are no new urls to scrape. \n",
    "\n",
    "Some of the urls do not link to movie reviews (they link to pages with content such as interviews and news articles) so the spider ignores these urls.\n",
    "\n",
    "1. Use Scrapy to get urls from www.moviesguide.org  \n",
    "    a. Navigate to this directory in the terminal and input the following commands:  \n",
    "`>> cd get_urls`  \n",
    "`>> scrapy crawl get_urls -o urls.json`  \n",
    "2. This creates a json file with urls of all of the movie reviews on the website.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Scrape the movie content data\n",
    "\n",
    "Now we will use Beautiful Soup to scrape the movie content data from the urls we just gathered.  There were a lot of errors in the urls scraped from the website.  For example, sometimes there was a typo in the url and it did not link to the correct website.  Beautiful Soup allowed me to iteratively gather urls, store the data in csv files, and the correct the url in the json file when I found a faulty url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(row):\n",
    "    ''' \n",
    "    get_rating(row)\n",
    "    \n",
    "    Return the content rating from a row in the table of content ratings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : list\n",
    "        A list of the html tags in the row.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    out : string\n",
    "        The content rating as a string.  Either 'None', 'Light', 'Medium', or 'Heavy'\n",
    "    '''\n",
    "    \n",
    "    rating_list = ['None', 'Light', 'Medium', 'Heavy']\n",
    "\n",
    "    for i in range(len(row)):\n",
    "        rating = row[i].div.attrs['class']\n",
    "        # Check each value in the row, 'movieguide_circle' is the default so ignore it\n",
    "        if rating[0] != 'movieguide_circle':\n",
    "            return rating_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genre(soup):\n",
    "    '''\n",
    "    get_genre(soup)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : Beautiful Soup object\n",
    "        The current website to be scraped.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    genre: string\n",
    "    rating: string\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    review_box = soup.find(class_='cb-review-box')\n",
    "    info = review_box.find_all('p')\n",
    "    genre_list = info[2].extract().text.split(':')[1:]\n",
    "    genre_list = [l.strip() for l in genre_list]\n",
    "    genre = ' '.join(genre_list)\n",
    "    genre = genre.split('/')[0]\n",
    "    \n",
    "    rating_list = info[4].text.split()[1:]\n",
    "    rating_list = [l.strip() for l in rating_list]\n",
    "    rating = ' '.join(rating_list)\n",
    "    \n",
    "    return genre, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(soup):\n",
    "    '''\n",
    "    get_content(soup)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : Beautiful Soup object\n",
    "        The current website to be scraped.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    content_dict : dict\n",
    "    \n",
    "    \n",
    "    Gathers the data from the get_rating and get_genre functions and returns the results as a dictionary. \n",
    "    '''\n",
    "    \n",
    "    content = soup.find(class_='movieguide_content_summary')\n",
    "    \n",
    "    # Check for exception when given a bad url.\n",
    "    try:\n",
    "        content_table = content.find_all('td')\n",
    "        language = content_table[1:5]\n",
    "        violence = content_table[6:10]\n",
    "        sex = content_table[11:15]\n",
    "        nudity = content_table[16:20]\n",
    "\n",
    "        # Create dictionary and put values in the dictionary\n",
    "        content_dict = {'Language': '', 'Violence': '', 'Sex': '', 'Nudity': ''}\n",
    "        content_dict['Language'] = get_rating(language)\n",
    "        content_dict['Violence'] = get_rating(violence)\n",
    "        content_dict['Sex'] = get_rating(sex)\n",
    "        content_dict['Nudity'] = get_rating(nudity)\n",
    "        content_dict['Title'] = (soup.find(class_=\"entry-title cb-entry-title cb-single-title\")\n",
    "                                    .text.strip()\n",
    "                                    .title())\n",
    "\n",
    "        genre, rating = get_genre(soup)\n",
    "        content_dict['Genre'] = genre\n",
    "        content_dict['Rating'] = rating\n",
    "\n",
    "        return content_dict\n",
    "    except:\n",
    "            print(\"Error here\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    '''\n",
    "    scrape(url)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        The current website to be scraped.\n",
    "        \n",
    "    Returns\n",
    "    ----------    \n",
    "    data : dict\n",
    "    \n",
    "    This function requests the html from the website, creates a Beautiful Soup object, \n",
    "    passes it to the function that scrapes the data and then returns the data.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    data = get_content(soup)\n",
    "\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to actually scrape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls.json', 'r') as jf:\n",
    "    all_urls = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dictionaries from each scrape url in a list.\n",
    "# When 100 movies have been scraped store as a csv file and restart.\n",
    "data_list = []\n",
    "n = 0 # Counter for files\n",
    "\n",
    "for url in all_urls:\n",
    "    print(len(data_list))\n",
    "    data = scrape(url['url'])\n",
    "    if data is not None:\n",
    "        data_list.append(data)\n",
    "    \n",
    "    if len(data_list) == 100:\n",
    "        pd.DataFrame(data_list).to_csv(f'data/scraped/scraped_{n}.csv')\n",
    "        print(f'scraped_{n}.csv saved')\n",
    "        data_list = []\n",
    "        n += 1\n",
    "        \n",
    "pd.DataFrame(data_list).to_csv(f'data/scraped/scraped_{n}.csv')\n",
    "print(f'scraped_{n}.csv saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Scrape the movie budgets and box office returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_numbers(url):\n",
    "    '''\n",
    "    scrape(url)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        The current website to be scraped.\n",
    "        \n",
    "    Returns\n",
    "    ---------- \n",
    "    df : DataFrame object\n",
    "    \n",
    "    This reads the data in the table from the html into a DataFrame.\n",
    "    The DataFrame columns are set and and the data is formatted.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    pd_table = pd.read_html(str(table))\n",
    "    df = pd.DataFrame(pd_table[0])\n",
    "    df.dropna(inplace=True)\n",
    "    df.columns = (['Index', 'Release_Date', 'Title', 'Production_Budget',\n",
    "                   'Domestic_Gross', 'Worldwide_Gross'])\n",
    "    df.drop(columns='Index', inplace=True)\n",
    "\n",
    "    \n",
    "    df['Production_Budget'] = df['Production_Budget'].str.replace('$', '')\n",
    "    df['Production_Budget'] = df['Production_Budget'].str.replace(',', '')\n",
    "    df['Domestic_Gross'] = df['Domestic_Gross'].str.replace('$', '')\n",
    "    df['Domestic_Gross'] = df['Domestic_Gross'].str.replace(',', '')\n",
    "    df['Worldwide_Gross'] = df['Worldwide_Gross'].str.replace('$', '')\n",
    "    df['Worldwide_Gross'] = df['Worldwide_Gross'].str.replace(',', '')\n",
    "    \n",
    "    df['Production_Budget'] = df['Production_Budget'].astype(int)\n",
    "    df['Domestic_Gross'] = df['Domestic_Gross'].astype(int)\n",
    "    df['Worldwide_Gross'] = df['Worldwide_Gross'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ([f'https://www.the-numbers.com/movie/budgets/all/{n+1}' \n",
    "         for n in range(0, 5600, 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrames are created 10 urls at a time and stored a csv file.\n",
    "\n",
    "data_list = pd.DataFrame()\n",
    "n = 0 # Counter for files\n",
    "f = 0\n",
    "\n",
    "for url in urls:\n",
    "#    print(url)\n",
    "    data = scrape_numbers(url)\n",
    "    \n",
    "    if data is not None:\n",
    "#        print('appending')\n",
    "        data_list = data_list.append(data, ignore_index=True)\n",
    "\n",
    "    if n == 9:\n",
    "        data_list.to_csv(f'data/budgets/budget_{f}.csv')\n",
    "        print(f'data/budgets/budget_{f}.csv file saved')\n",
    "        data_list = pd.DataFrame()\n",
    "        n = -1\n",
    "        f += 1\n",
    "    n += 1\n",
    "\n",
    "pd.DataFrame(data_list).to_csv(f'data/budgets/budget_{f}.csv')\n",
    "print(f'data/budgets/budget_{f}.csv file saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
